# -*- coding: utf-8 -*-
"""Stroke_Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pqmt1KKhan6ewbeMEmFhre6wDlfWrvJv

<a href="https://colab.research.google.com/github/parisazeynaly/Stroke-prediction/blob/main/Stroke_Prediction.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>

## Stroke Prediction

#### Our top priority in this health problem is to identify patients with a stroke.

According to the World Health Organization (WHO) stroke is the 2nd leading cause of death globally, responsible for approximately 11% of total deaths.<br>
This dataset is used to predict whether a patient is likely to get stroke based on the input parameters like gender, age, various diseases, and smoking status. Each row in the data provides relavant information about the patient.

<b>Attribute Information</b>

1) id: unique identifier<br>
2) gender: "Male", "Female" or "Other"<br>
3) age: age of the patient<br>
4) hypertension: 0 if the patient doesn't have hypertension, 1 if the patient has hypertension<br>
5) heart_disease: 0 if the patient doesn't have any heart diseases, 1 if the patient has a heart disease<br>
6) ever_married: "No" or "Yes"<br>
7) work_type: "children", "Govt_jov", "Never_worked", "Private" or "Self-employed"<br>
8) Residence_type: "Rural" or "Urban"<br>
9) avg_glucose_level: average glucose level in blood<br>
10) bmi: body mass index<br>
11) smoking_status: "formerly smoked", "never smoked", "smokes" or "Unknown"*<br>
12) stroke: 1 if the patient had a stroke or 0 if not<br>
*Note: "Unknown" in smoking_status means that the information is unavailable for this patient<br>
"""

!git clone https://github.com/parisazeynaly/Stroke-prediction.git

"""### Import libraries"""

import pandas as pd
import numpy as np
import joblib
import logging
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.impute import KNNImputer
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, accuracy_score
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier

# Load the dataset
df = pd.read_csv("healthcare-dataset-stroke-data.csv")

print("--- First 5 rows of the dataset ---")
print(df.head())

print("\n--- Dataset Shape ---")
print(f"Rows: {df.shape[0]}, Columns: {df.shape[1]}")

print("\n--- Dataset Information ---")
df.info()

print("\n--- Descriptive Statistics ---")
print(df.describe())

print("\n--- Missing Values Count per Column ---")
print(df.isna().sum())

print("\n--- Column Names ---")
print(df.columns.tolist())

### 3. Handle Missing Values (BMI)

# The 'bmi' column has missing values, which will be imputed using KNNImputer.
imputer = KNNImputer(n_neighbors=5)
df['bmi'] = imputer.fit_transform(df[['bmi']])

print("\n--- Missing Values after BMI Imputation ---")
print(df.isna().sum())

# from sklearn.impute import KNNImputer

# imputer = KNNImputer(n_neighbors = 5)
# df['bmi'] = imputer.fit_transform(df[['bmi']])

"""### Exploratory Data Analysis"""

### 4. Exploratory Data Analysis (EDA)

# EDA helps to analyze and investigate data sets and summarize their main characteristics,
# often employing data visualization methods.
# Libraries for visualization (EDA)
import matplotlib.pyplot as plt
import seaborn as sns
print("--- Stroke vs. Non-Stroke Distribution ---")
fig, axes = plt.subplots(figsize=(8, 4))
df['stroke'].value_counts(normalize=True).plot.bar(width=0.2, color=('red','green'))
plt.title('Proportion of Stroke Cases')
plt.xlabel('Stroke (0: No, 1: Yes)')
plt.ylabel('Proportion')
plt.tight_layout()
plt.show()

"""Exploratory data analysis (EDA) is used to analyze and investigate data sets and summarize their main characteristics, often employing data visualization methods."""

cols = df[['age', 'hypertension', 'heart_disease', 'avg_glucose_level', 'bmi']]
cols.head()

# lets see how data is distributed for every column
import seaborn as sns
plt.figure(figsize=(20,30), facecolor='white')
plotnumber=1

for column in cols:
    if plotnumber<=15:
        ax = plt.subplot(5,3,plotnumber)
        sns.distplot(cols[column])
        plt.xlabel(column, fontsize=20)

    plotnumber+=1
plt.tight_layout()

# Optional: For comprehensive data profiling
# !pip install pandas_profiling[notebook] # Uncomment and run if you want to generate the profile report
# from pandas_profiling import ProfileReport

# Selecting numerical columns for distribution and outlier analysis
numerical_cols = ['age', 'hypertension', 'heart_disease', 'avg_glucose_level', 'bmi']

print("\n--- Distribution of Numerical Features (Histogram/KDE) ---")
plt.figure(figsize=(20, 15)) # Adjusted figure size for better display
plotnumber = 1
for column in numerical_cols:
    if plotnumber <= len(numerical_cols):
        ax = plt.subplot(3, 2, plotnumber) # Adjusted subplot grid
        sns.histplot(df[column], kde=True, ax=ax)
        plt.xlabel(column, fontsize=15)
        plt.ylabel('Density' if 'kde' in str(type(ax)) else 'Count', fontsize=12) # Dynamic label
    plotnumber += 1
plt.tight_layout()
plt.show()

print("\n--- Outlier Detection for Numerical Features (Box Plots) ---")
plt.figure(figsize=(20, 15)) # Adjusted figure size
plotnumber = 1
for column in numerical_cols:
    if plotnumber <= len(numerical_cols):
        ax = plt.subplot(3, 2, plotnumber) # Adjusted subplot grid
        sns.boxplot(y=df[column], ax=ax) # Use y for vertical box plot
        plt.ylabel(column, fontsize=15)
    plotnumber += 1
plt.tight_layout()
plt.show()

print("\n--- Distribution of Categorical Features (Count Plots) ---")

# The commented-out barplot code was good, let's include a similar one.

categorical_features = ['gender', 'ever_married', 'work_type', 'Residence_type', 'smoking_status']

plt.figure(figsize=(20, 12)) # Adjusted figure size
plotnumber = 1
for column in categorical_features:
    if plotnumber <= len(categorical_features):
        ax = plt.subplot(2, 3, plotnumber) # Adjusted subplot grid
        sns.countplot(x=df[column], ax=ax, palette='viridis')
        plt.xlabel(column, fontsize=15)
        plt.ylabel('Count', fontsize=12)
        plt.xticks(rotation=45, ha='right') # Rotate labels for better readability
    plotnumber += 1
plt.tight_layout()
plt.show()


print("\n--- Relation between Categorical Features and Target (Stroke) ---")

plt.figure(figsize=(20, 12))
plotnumber = 1
for column in categorical_features:
    if plotnumber <= len(categorical_features):
        ax = plt.subplot(2, 3, plotnumber)
        sns.barplot(x=df[column], y=df['stroke'], ax=ax, palette='coolwarm')
        plt.xlabel(column, fontsize=15)
        plt.ylabel('Stroke Probability', fontsize=12)
        plt.xticks(rotation=45, ha='right')
    plotnumber += 1
plt.tight_layout()
plt.show()

# lets see outliers
# import seaborn as sns
# plt.figure(figsize=(20,30), facecolor='white')
# plotnumber=1

# for column in cols:
#     if plotnumber<=15:
#         ax = plt.subplot(5,3,plotnumber)
#         sns.boxplot(cols[column])
#         plt.xlabel(column, fontsize=20)

#     plotnumber+=1
# plt.tight_layout()

### 5. Feature Engineering: Handling Categorical Columns

# One-hot encoding for categorical features
# 'gender' column has an 'Other' category with very few samples,
# which might be dropped or handled specifically if it causes issues.
# For now, keeping as is but considering drop_first=True to avoid multicollinearity.

gender_encoded = pd.get_dummies(df[['gender']], prefix='gender', drop_first=True)
married_encoded = pd.get_dummies(df[['ever_married']], prefix='married', drop_first=True)
work_encoded = pd.get_dummies(df[['work_type']], prefix='work', drop_first=True)
residence_encoded = pd.get_dummies(df[['Residence_type']], prefix='residence', drop_first=True)
smoking_encoded = pd.get_dummies(df[['smoking_status']], prefix='smoking', drop_first=True)

# Combine original numerical features with new encoded categorical features
data = pd.concat([
    df.drop(columns=['gender', 'ever_married', 'work_type', 'Residence_type', 'smoking_status', 'id'], axis=1),
    gender_encoded,
    married_encoded,
    work_encoded,
    residence_encoded,
    smoking_encoded
], axis=1)

print("\n--- Data after One-Hot Encoding and dropping original categorical/id columns ---")
print(data.head())
print(f"New shape: {data.shape}")
print(f"New columns: {data.columns.tolist()}")

### 6. Correlation Analysis

# Analyze the correlation of features with the target variable 'stroke'
print("\n--- Correlation of Features with 'stroke' ---")
corr = data.corr(numeric_only=True)['stroke'].sort_values(ascending=False).to_frame()
plt.figure(figsize=(2, 8))
sns.heatmap(corr, cmap='Blues', cbar=False, annot=True, fmt=".2f") # fmt=".2f" for 2 decimal places
plt.title('Feature Correlation with Stroke')
plt.show()

### 7. Data Splitting and Scaling

# Define features (x) and target (y)
x = data.drop("stroke", axis=1)
y = data[['stroke']]

print(f"\nShape of features (x): {x.shape}")
print(f"Shape of target (y): {y.shape}")

# Standardize the numerical features
# Apply StandardScaler to the entire feature set (x) after one-hot encoding.
# This ensures all numerical features are scaled consistently.
scaler = StandardScaler()
x_scaled = scaler.fit_transform(x)

# Convert scaled features back to a DataFrame for easier inspection, maintaining column names
x_scaled_df = pd.DataFrame(x_scaled, columns=x.columns)
print("\n--- First 5 rows of Scaled Features (x) ---")
print(x_scaled_df.head())

# Split the dataset into training and testing sets
x_train, x_test, y_train, y_test = train_test_split(x_scaled, y, test_size=0.33, random_state=42, stratify=y) # Added stratify for imbalanced target

print(f"\nShape of x_train: {x_train.shape}")
print(f"Shape of x_test: {x_test.shape}")
print(f"Shape of y_train: {y_train.shape}")
print(f"Shape of y_test: {y_test.shape}")

### 8. Model Training and Evaluation (With MLflow)
!pip install mlflow
import mlflow
import mlflow.sklearn
import mlflow
import mlflow.sklearn
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score, classification_report, ConfusionMatrixDisplay
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
import pickle

# Define your models
models = {
    'Decision Tree': DecisionTreeClassifier(random_state=1),
    'Random Forest': RandomForestClassifier(random_state=42),
    'K-Nearest Neighbors': KNeighborsClassifier(),
    'Support Vector Machine': SVC(probability=True),
    'Logistic Regression': LogisticRegression(max_iter=1000)
}

# Set experiment
mlflow.set_experiment("Stroke Prediction Experiment")

best_accuracy = 0
best_model = None
best_model_name = ""

for name, model in models.items():
    with mlflow.start_run(run_name=name):
        # Train
        model.fit(x_train, y_train.values.ravel())

        # Predict
        y_pred = model.predict(x_test)

        # Metrics
        acc = accuracy_score(y_test, y_pred)
        report = classification_report(y_test, y_pred, output_dict=True)

        # Log params & metrics
        mlflow.log_param("model_type", name)
        mlflow.log_metric("accuracy", acc)

        for label, scores in report.items():
            if isinstance(scores, dict):
                for metric_name, value in scores.items():
                    mlflow.log_metric(f"{label}_{metric_name}", value)

        # Save Confusion Matrix
        disp = ConfusionMatrixDisplay.from_predictions(y_test, y_pred)
        disp.figure_.suptitle(f"Confusion Matrix: {name}")
        cm_filename = f"conf_matrix_{name}.png"
        plt.savefig(cm_filename)
        plt.close()
        mlflow.log_artifact(cm_filename)

        # Track best model
        if acc > best_accuracy:
            best_accuracy = acc
            best_model = model
            best_model_name = name

# âœ… Register the best model
with mlflow.start_run(run_name="Register_Best_Model"):
    mlflow.log_param("best_model_name", best_model_name)
    mlflow.log_metric("best_accuracy", best_accuracy)

    mlflow.sklearn.log_model(
        sk_model=best_model,
        artifact_path="model",
        registered_model_name="StrokePredictionModel"
    )

print(f"\nâœ… Registered best model ({best_model_name}) with accuracy: {best_accuracy:.4f}")

# âœ… Save model and scaler for app usage
with open("model_pickle.pkl", 'wb') as f:
    pickle.dump(best_model, f)
with open("scaler.pkl", 'wb') as f:
    pickle.dump(scaler, f)


# print("--- Training and Evaluating Different Models with MLflow ---")

# # Initialize MLflow experiment
# mlflow.set_experiment("Stroke Prediction Experiment")

# # Dictionary of models to evaluate
# models = {
#     'Decision Tree': DecisionTreeClassifier(random_state=1),
#     'Random Forest': RandomForestClassifier(random_state=42),
#     'K-Nearest Neighbors': KNeighborsClassifier(),
#     'Support Vector Machine': SVC(probability=True),
#     'Logistic Regression': LogisticRegression(max_iter=1000)
# }

# for name, model in models.items():
#     with mlflow.start_run(run_name=name):
#         # Train the model
#         model.fit(x_train, y_train.values.ravel())

#         # Make predictions
#         y_pred = model.predict(x_test)

#         # Evaluate the model
#         acc = accuracy_score(y_test, y_pred)
#         report = classification_report(y_test, y_pred, output_dict=True)

#         # Log parameters, metrics, and model
#         mlflow.log_param("model_type", name)
#         mlflow.log_metric("accuracy", acc)

#         # Log each class precision, recall, and f1-score
#         for label, scores in report.items():
#             if isinstance(scores, dict):  # skip 'accuracy' which is a float
#                 for metric_name, value in scores.items():
#                     mlflow.log_metric(f"{label}_{metric_name}", value)

#         # Log model
#         mlflow.sklearn.log_model(model, f"{name}_model")

#         # Optional: Save and log confusion matrix as an artifact
#         from sklearn.metrics import ConfusionMatrixDisplay
#         import matplotlib.pyplot as plt

#         disp = ConfusionMatrixDisplay.from_predictions(y_test, y_pred)
#         disp.figure_.suptitle(f"Confusion Matrix: {name}")
#         cm_filename = f"conf_matrix_{name}.png"
#         plt.savefig(cm_filename)
#         plt.close()
#         mlflow.log_artifact(cm_filename)

# print("\nâœ… All models trained and logged to MLflow. Run `mlflow ui` to visualize results.")


# from sklearn.metrics import ConfusionMatrixDisplay
# import matplotlib.pyplot as plt

# # Set experiment
# mlflow.set_experiment("Stroke Prediction Experiment")

# for name, model in models.items():
#     with mlflow.start_run(run_name=name):

#         # Train the model
#         model.fit(x_train, y_train.values.ravel())

#         # Predict
#         y_pred = model.predict(x_test)

#         # Metrics
#         acc = accuracy_score(y_test, y_pred)
#         report = classification_report(y_test, y_pred, output_dict=True)

#         # Log params and metrics
#         mlflow.log_param("model_type", name)
#         mlflow.log_metric("accuracy", acc)

#         for label, scores in report.items():
#             if isinstance(scores, dict):
#                 for metric_name, value in scores.items():
#                     mlflow.log_metric(f"{label}_{metric_name}", value)

#         # âœ… Log model + register it in MLflow Model Registry
#         mlflow.sklearn.log_model(
#             sk_model=model,
#             artifact_path="model",
#             registered_model_name="StrokePredictionModel"  # <- Model registry name
#         )

#         # âœ… Save Confusion Matrix
#         disp = ConfusionMatrixDisplay.from_predictions(y_test, y_pred)
#         disp.figure_.suptitle(f"Confusion Matrix: {name}")
#         cm_filename = f"conf_matrix_{name}.png"
#         plt.savefig(cm_filename)
#         plt.close()

#         mlflow.log_artifact(cm_filename)

# print("\nâœ… All models trained, tracked, and registered in MLflow.")



### 9. Save the Best Model

# It's good practice to save the best performing model. In this case, the GridSearchCV's best estimator.
import pickle
MODEL_FILENAME = 'model_pickle.pkl'

# In this case, saving the last trained 'model' as 'best_svc_model' was not defined.
# If hyperparameter tuning is performed, the best model should be saved here.
with open(MODEL_FILENAME, 'wb') as file:
    pickle.dump(model, file) # Changed from best_svc_model to model

print(f"\nBest model (SVC) saved to '{MODEL_FILENAME}'")

# You might also want to save the scaler used for preprocessing, as it's needed for new predictions.
SCALER_FILENAME = 'scaler.pkl'
with open(SCALER_FILENAME, 'wb') as file:
    pickle.dump(scaler, file)
print(f"Scaler saved to '{SCALER_FILENAME}'")

# --- Begin Security Testing ---
import numpy as np
from sklearn.metrics import accuracy_score

def add_adversarial_noise(X, epsilon=0.05):
    np.random.seed(42)
    noise = epsilon * np.sign(np.random.randn(*X.shape))
    return X + noise

x_test_adv = add_adversarial_noise(x_test)
y_pred_adv = model.predict(x_test_adv)

adv_acc = accuracy_score(y_test, y_pred_adv)
print(f"ðŸ” Adversarial Test Accuracy: {adv_acc:.4f}")

import mlflow

with mlflow.start_run(run_name="Security_Test_Adversarial"):
    mlflow.log_param("attack_type", "random_noise")
    mlflow.log_param("epsilon", 0.05)
    mlflow.log_metric("adv_accuracy", adv_acc)
    mlflow.sklearn.log_model(model, "adversarial_model")

    print("âœ… Adversarial test logged to MLflow.")